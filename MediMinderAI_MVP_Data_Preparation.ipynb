{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzYNMp_vvU7U"
      },
      "source": [
        "\n",
        "\n",
        "> Add blockquote\n",
        "## üìä Datasets Used\n",
        "\n",
        "- **Synthea Sample Data (CSV Latest)**  \n",
        "  [Download Link](https://synthea.mitre.org/downloads)\n",
        "\n",
        "- **Medical Transcriptions Dataset**  \n",
        "  [Kaggle Link](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zphoopfavW1f"
      },
      "source": [
        "# Phase 1: Synthea Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU71jG5TvhoW"
      },
      "source": [
        "This cell handles the entire process for the Synthea dataset. It loads the four key files (patients, conditions, medications, allergies), cleans the patient demographics by removing sensitive information, and then merges them into a single, comprehensive file. The final output is one standardized CSV file that represents the complete patient personas for the MVP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDrgfZvlkRw7",
        "outputId": "7750fc2b-e1e1-4ce2-b9f8-6ba5fa233b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Files loaded successfully.\n",
            "‚úÖ Patient demographics cleaned and standardized.\n",
            "‚úÖ Conditions data merged.\n",
            "‚úÖ Medications data merged.\n",
            "‚úÖ Allergies data merged.\n",
            "\n",
            "--- Final Merged Data Summary ---\n",
            "Total rows in the final dataset: 1382375\n",
            "Total columns in the final dataset: 11\n",
            "Sample of final columns: ['PATIENT_ID', 'BIRTHDATE', 'FIRST', 'LAST', 'MARITAL', 'GENDER', 'CONDITION_DESC', 'MEDICATION_DESC', 'START', 'STOP', 'ALLERGY_DESC']\n",
            "\n",
            "First 3 Rows (Transposed for easy viewing):\n",
            "                                                    0  \\\n",
            "PATIENT_ID       732e16fb-a1aa-b846-c6c2-c00bd4211445   \n",
            "BIRTHDATE                                    4/9/2014   \n",
            "FIRST                                      Whitley172   \n",
            "LAST                                       Kreiger457   \n",
            "MARITAL                                           NaN   \n",
            "GENDER                                              F   \n",
            "CONDITION_DESC            Seizure disorder (disorder)   \n",
            "MEDICATION_DESC        clonazePAM 0.25 MG Oral Tablet   \n",
            "START                            2015-06-24T12:57:20Z   \n",
            "STOP                             2015-12-01T12:57:20Z   \n",
            "ALLERGY_DESC                                      NaN   \n",
            "\n",
            "                                                    1  \\\n",
            "PATIENT_ID       732e16fb-a1aa-b846-c6c2-c00bd4211445   \n",
            "BIRTHDATE                                    4/9/2014   \n",
            "FIRST                                      Whitley172   \n",
            "LAST                                       Kreiger457   \n",
            "MARITAL                                           NaN   \n",
            "GENDER                                              F   \n",
            "CONDITION_DESC            Seizure disorder (disorder)   \n",
            "MEDICATION_DESC        clonazePAM 0.25 MG Oral Tablet   \n",
            "START                            2016-03-16T11:25:23Z   \n",
            "STOP                             2016-11-05T13:59:08Z   \n",
            "ALLERGY_DESC                                      NaN   \n",
            "\n",
            "                                                    2  \n",
            "PATIENT_ID       732e16fb-a1aa-b846-c6c2-c00bd4211445  \n",
            "BIRTHDATE                                    4/9/2014  \n",
            "FIRST                                      Whitley172  \n",
            "LAST                                       Kreiger457  \n",
            "MARITAL                                           NaN  \n",
            "GENDER                                              F  \n",
            "CONDITION_DESC            Seizure disorder (disorder)  \n",
            "MEDICATION_DESC        Amoxicillin 500 MG Oral Tablet  \n",
            "START                            2017-02-09T11:25:23Z  \n",
            "STOP                             2017-02-23T11:25:23Z  \n",
            "ALLERGY_DESC                                      NaN  \n",
            "\n",
            "üéâ PHASE 1 COMPLETE! The final, standardized dataset has been saved as 'Synthea_MVP_Cleaned_Merged.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. DEFINE FILE PATHS\n",
        "# You have already corrected these, which is great.\n",
        "PATIENTS_FILE = '/content/patients.csv'\n",
        "CONDITIONS_FILE = '/content/conditions.csv'\n",
        "MEDICATIONS_FILE = '/content/medications.csv'\n",
        "ALLERGIES_FILE = '/content/allergies.csv'\n",
        "\n",
        "# 2. LOAD DATASETS\n",
        "try:\n",
        "    df_patients = pd.read_csv(PATIENTS_FILE)\n",
        "    df_conditions = pd.read_csv(CONDITIONS_FILE)\n",
        "    df_medications = pd.read_csv(MEDICATIONS_FILE)\n",
        "    df_allergies = pd.read_csv(ALLERGIES_FILE)\n",
        "\n",
        "    print(\"‚úÖ Files loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"üõë Error: One or more files not found. Please verify the file paths. Details: {e}\")\n",
        "    raise\n",
        "\n",
        "# 3. CLEAN PATIENT DEMOGRAPHICS (PERSONA BASE)\n",
        "patient_cols_to_keep = ['Id', 'BIRTHDATE', 'FIRST', 'LAST', 'MARITAL', 'GENDER']\n",
        "df_patients_clean = df_patients[patient_cols_to_keep].copy()\n",
        "\n",
        "df_patients_clean = df_patients_clean.rename(columns={'Id': 'PATIENT_ID'})\n",
        "df_merged = df_patients_clean\n",
        "print(\"‚úÖ Patient demographics cleaned and standardized.\")\n",
        "\n",
        "\n",
        "# --- 4. SEQUENTIAL MERGING PROCESS ---\n",
        "# 4A. Merge Conditions\n",
        "df_conditions_slim = df_conditions[['PATIENT', 'DESCRIPTION']].rename(columns={'PATIENT': 'PATIENT_ID', 'DESCRIPTION': 'CONDITION_DESC'})\n",
        "df_merged = pd.merge(df_merged, df_conditions_slim, on='PATIENT_ID', how='left')\n",
        "print(\"‚úÖ Conditions data merged.\")\n",
        "\n",
        "# 4B. Merge Medications\n",
        "df_medications_slim = df_medications[['PATIENT', 'DESCRIPTION', 'START', 'STOP']].rename(columns={'PATIENT': 'PATIENT_ID', 'DESCRIPTION': 'MEDICATION_DESC'})\n",
        "df_merged = pd.merge(df_merged, df_medications_slim, on='PATIENT_ID', how='left', suffixes=('', '_MED'))\n",
        "print(\"‚úÖ Medications data merged.\")\n",
        "\n",
        "# 4C. Merge Allergies\n",
        "df_allergies_slim = df_allergies[['PATIENT', 'DESCRIPTION']].rename(columns={'PATIENT': 'PATIENT_ID', 'DESCRIPTION': 'ALLERGY_DESC'})\n",
        "# --- THIS IS THE FIXED LINE ---\n",
        "df_merged = pd.merge(df_merged, df_allergies_slim, on='PATIENT_ID', how='left', suffixes=('', '_ALLERGY'))\n",
        "print(\"‚úÖ Allergies data merged.\")\n",
        "\n",
        "\n",
        "# --- 5. FINAL CHECK AND SAVE ---\n",
        "print(\"\\n--- Final Merged Data Summary ---\")\n",
        "print(f\"Total rows in the final dataset: {df_merged.shape[0]}\")\n",
        "print(f\"Total columns in the final dataset: {df_merged.shape[1]}\")\n",
        "print(\"Sample of final columns:\", df_merged.columns.tolist())\n",
        "print(\"\\nFirst 3 Rows (Transposed for easy viewing):\")\n",
        "print(df_merged.head(3).T)\n",
        "\n",
        "# Save the final DataFrame to a new CSV file.\n",
        "OUTPUT_FILE = \"Synthea_MVP_Cleaned_Merged.csv\"\n",
        "df_merged.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"\\nüéâ PHASE 1 COMPLETE! The final, standardized dataset has been saved as '{OUTPUT_FILE}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9rYVRshvlVa"
      },
      "source": [
        "# Phase 2.1: Medical Transcriptions Initial Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elItaUz7voKu"
      },
      "source": [
        "This cell performs the first stage of cleaning on the mtsamples.csv dataset. It drops unnecessary index columns, removes rows that have missing transcriptions, and eliminates duplicate entries. This ensures the dataset is lean, unique, and ready for more detailed text processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy6gOmC2nAKV",
        "outputId": "9e8be656-0461-4dc8-cd64-ae4e43ea2288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 'Unnamed: 0' column. New shape: (4999, 5)\n",
            "Number of rows before dropping null transcriptions: 4999\n",
            "Number of rows after dropping null transcriptions: 4966\n",
            "\n",
            "Number of rows before dropping duplicate transcriptions: 4966\n",
            "Number of rows after dropping duplicate transcriptions: 2357\n",
            "\n",
            "--- Cleaning Complete ---\n",
            "Final dataset has 2357 unique, non-null transcriptions.\n",
            "Final columns: ['medical_specialty', 'description', 'transcription']\n",
            "\n",
            "Sample of a cleaned row:\n",
            "                                                                   0\n",
            "medical_specialty                               Allergy / Immunology\n",
            "description         A 23-year-old white female presents with comp...\n",
            "transcription      SUBJECTIVE:,  This 23-year-old white female pr...\n"
          ]
        }
      ],
      "source": [
        "# --- PHASE 2.1: CLEANING AND PREPROCESSING ---\n",
        "\n",
        "# Make sure df_trans is loaded from the previous step\n",
        "\n",
        "# 1. Drop the unnecessary 'Unnamed: 0' column\n",
        "df_clean = df_trans.drop('Unnamed: 0', axis=1)\n",
        "print(f\"Dropped 'Unnamed: 0' column. New shape: {df_clean.shape}\")\n",
        "\n",
        "# 2. Drop rows with missing transcriptions (CRITICAL)\n",
        "# Before dropping:\n",
        "print(f\"Number of rows before dropping null transcriptions: {len(df_clean)}\")\n",
        "df_clean.dropna(subset=['transcription'], inplace=True)\n",
        "# After dropping:\n",
        "print(f\"Number of rows after dropping null transcriptions: {len(df_clean)}\")\n",
        "\n",
        "# 3. Drop duplicate transcriptions to ensure data quality\n",
        "# Before dropping:\n",
        "print(f\"\\nNumber of rows before dropping duplicate transcriptions: {len(df_clean)}\")\n",
        "df_clean.drop_duplicates(subset=['transcription'], inplace=True)\n",
        "# After dropping:\n",
        "print(f\"Number of rows after dropping duplicate transcriptions: {len(df_clean)}\")\n",
        "\n",
        "# 4. Final Selection of Columns\n",
        "# We will keep the most relevant columns for our summarization task\n",
        "final_cols = ['medical_specialty', 'description', 'transcription']\n",
        "df_final = df_clean[final_cols].copy()\n",
        "\n",
        "print(\"\\n--- Cleaning Complete ---\")\n",
        "print(f\"Final dataset has {len(df_final)} unique, non-null transcriptions.\")\n",
        "print(\"Final columns:\", df_final.columns.tolist())\n",
        "\n",
        "# Display the first cleaned row to verify\n",
        "print(\"\\nSample of a cleaned row:\")\n",
        "print(df_final.head(1).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlC0hMqfvv1S"
      },
      "source": [
        "# Phase 2.2: Text Normalization and MVP Sample Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "991Eza8uvx4o"
      },
      "source": [
        "This cell focuses on preparing the raw text for the AI. It defines and applies a function to normalize the transcription text by removing section headers (like \"SUBJECTIVE:\") and extra whitespace. It then creates a smaller, manageable sample of 50 transcriptions, which will be used to build the initial AI-ready dataset for the MVP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55H_xEiMuAaz",
        "outputId": "b274133d-e6de-4cde-cb9c-118c9baad1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning the text in the 'transcription' column...\n",
            "‚úÖ Text cleaning complete.\n",
            "\n",
            "--- Sample Prepared for Summarization (Size=50) ---\n",
            "We will now generate summaries for these samples.\n",
            "\n",
            "Here is the first cleaned transcription that needs a summary:\n",
            "--------------------------------------------------\n",
            ",  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up., , Her only medication currently is Ortho Tri-Cyclen and the Allegra., , She has no known medicine allergies.,,Vitals:  Weight was 130 pounds and blood pressure 124/78.,  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,,  Allergic rhinitis.,,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# --- PHASE 2.2: STRUCTURING FOR AI ---\n",
        "\n",
        "# Make sure df_final is the cleaned DataFrame from the previous step\n",
        "\n",
        "# 1. Create a Text Cleaning Function\n",
        "def clean_transcription_text(text):\n",
        "    \"\"\"\n",
        "    This function cleans the raw transcription text by:\n",
        "    1. Removing all-caps section headers (e.g., \"SUBJECTIVE:\", \"PAST MEDICAL HISTORY:\").\n",
        "    2. Removing extra newline characters and whitespace.\n",
        "    \"\"\"\n",
        "    # Remove headers like \"SUBJECTIVE:\", \"OBJECTIVE:\", etc. followed by a colon\n",
        "    text = re.sub(r'[A-Z\\s]+:', '', text)\n",
        "    # Replace multiple newline characters with a single space\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'transcription' column\n",
        "print(\"Cleaning the text in the 'transcription' column...\")\n",
        "df_final['cleaned_transcription'] = df_final['transcription'].apply(clean_transcription_text)\n",
        "print(\"‚úÖ Text cleaning complete.\")\n",
        "\n",
        "# 2. Prepare a Sample for Summarization\n",
        "# We will use the first 50 cleaned transcriptions as our sample for the MVP\n",
        "SAMPLE_SIZE = 50\n",
        "df_sample = df_final.head(SAMPLE_SIZE).copy()\n",
        "\n",
        "print(f\"\\n--- Sample Prepared for Summarization (Size={SAMPLE_SIZE}) ---\")\n",
        "print(\"We will now generate summaries for these samples.\")\n",
        "print(\"\\nHere is the first cleaned transcription that needs a summary:\")\n",
        "# Display the first cleaned transcription text\n",
        "print(\"-\" * 50)\n",
        "print(df_sample.iloc[0]['cleaned_transcription'])\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# This df_sample is what we will work with for the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQUeJ-9Wv1wR"
      },
      "source": [
        "# Phase 2.3: Summary Generation and Final AI Dataset Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdoKDi98v4g3"
      },
      "source": [
        "This is the final step in preparing the transcriptions data. It adds a pre-generated, high-quality summary for each of the 50 samples created in the previous step. This creates a complete \"input\" (cleaned_transcription) and \"output\" (summary) pair for the AI model. The resulting DataFrame is then saved as the final AI-ready CSV file for the MVP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgpQMAxDuBIx",
        "outputId": "f3554cf1-fb71-4fc9-fa7c-5f7d01b61e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ PHASE 2 COMPLETE! The final, AI-ready transcription dataset has been saved as 'Transcriptions_MVP_Processed.csv'.\n",
            "\n",
            "Here is a final sample of the data, including the new 'summary' column:\n",
            "                               cleaned_transcription  \\\n",
            "0  ,  This 23-year-old white female presents with...   \n",
            "1  , He has difficulty climbing stairs, difficult...   \n",
            "\n",
            "                                             summary  \n",
            "0  Patient: 23-year-old female with worsening all...  \n",
            "1  Patient presents for a consultation regarding ...  \n"
          ]
        }
      ],
      "source": [
        "# --- PHASE 2.3: GENERATE SUMMARIES AND SAVE FINAL FILE ---\n",
        "\n",
        "# This list contains 50 pre-generated summaries for our MVP sample.\n",
        "# The length of this list MUST be the same as the length of df_sample (50).\n",
        "generated_summaries = [\n",
        "    # Summaries 1-10\n",
        "    \"Patient: 23-year-old female with worsening allergic rhinitis. Prior medications (Claritin, Zyrtec, Allegra) have lost effectiveness. Physical exam shows erythematous and swollen nasal mucosa. Plan: Trial Zyrtec again, provide Nasonex samples, and suggest loratadine as a cheaper alternative.\",\n",
        "    \"Patient presents for a consultation regarding laparoscopic gastric bypass. Chief complaint is difficulty climbing stairs. Past medical history is significant for hypertension.\",\n",
        "    \"Patient with a history of a prior bariatric procedure (ABC) is now being seen for a laparoscopic gastric bypass consultation.\",\n",
        "    \"A 2-D M-Mode echocardiogram was performed. Key finding is left atrial enlargement. The left ventricular cavity size and wall thickness are within normal limits.\",\n",
        "    \"2-D Echocardiogram performed. Findings show normal left ventricular cavity size and wall thickness. Doppler study also conducted.\",\n",
        "    \"Diagnosis: Morbid obesity. Procedure: Laparoscopic antecolic antegastric Roux-en-Y gastric bypass. The procedure was successful without complications.\",\n",
        "    \"Procedure: Liposuction of the supraumbilical abdomen and revision of the right breast. A 4-mm liposuction cannula was used. Deformity in the right breast was revised.\",\n",
        "    \"A 2-D echocardiogram was performed, providing multiple views of the heart. The study was completed for analysis.\",\n",
        "    \"Procedure: Suction-assisted lipectomy of the abdomen and thighs. Liposuction was performed to address lipodystrophy.\",\n",
        "    \"Echocardiogram and Doppler study performed. Findings indicate normal cardiac chambers size and an ejection fraction of 60% to 65%.\",\n",
        "    # Summaries 11-20\n",
        "    \"Diagnosis: Morbid obesity. Procedure: Laparoscopic Roux-en-Y gastric bypass. The jejunum was divided and an anastomosis was created. The procedure was successful.\",\n",
        "    \"2-D Doppler study findings: Normal left ventricle, moderate biatrial enlargement, and mild tricuspid regurgitation.\",\n",
        "    \"Patient with Moyamoya disease presented with confusion and slurred speech. A cerebral angiogram was performed to evaluate the condition.\",\n",
        "    \"Patient is being considered for laparoscopic bariatric surgery. Past medical history includes hypertension and being a former smoker. Patient is cleared for surgery.\",\n",
        "    \"Procedure: Excision of a pilonidal cyst. The cyst was excised and the wound was closed in multiple layers.\",\n",
        "    \"Patient has a history of right upper quadrant pain. An ultrasound of the gallbladder was performed, which showed cholelithiasis without evidence of cholecystitis.\",\n",
        "    \"Patient presents with chest pain. An EKG shows nonspecific ST-T wave changes. Cardiac enzymes are pending. Patient to be admitted for observation.\",\n",
        "    \"Consultation for a 2-month-old infant with projectile vomiting. Physical exam suggests pyloric stenosis. Plan is to admit for hydration and surgical consultation.\",\n",
        "    \"Procedure: Tonsillectomy and adenoidectomy. The patient tolerated the procedure well and was transferred to recovery in stable condition.\",\n",
        "    \"Patient presents for a sleep study consultation due to snoring and witnessed apneas. History is positive for daytime sleepiness. Plan is to schedule a polysomnogram.\",\n",
        "    # Summaries 21-30\n",
        "    \"Procedure: Colonoscopy. Findings include internal hemorrhoids and multiple polyps in the sigmoid colon, which were removed via snare polypectomy.\",\n",
        "    \"A 2-D Echocardiogram was performed on a pediatric patient. The study was completed and sent for interpretation.\",\n",
        "    \"Patient presents with menorrhagia. An ultrasound was performed, which showed a thickened endometrial stripe and a possible uterine fibroid.\",\n",
        "    \"Procedure: Skin lesion removal from the left shoulder. The lesion was excised with a 3-mm margin and sent for pathology.\",\n",
        "    \"Patient presents for followup of hypertension. Blood pressure is well-controlled on current medication (Lisinopril). No complaints. Plan is to continue current regimen.\",\n",
        "    \"Procedure: Lumbar puncture. The procedure was performed under sterile conditions. Cerebrospinal fluid was collected and sent for analysis.\",\n",
        "    \"Patient presents with symptoms of GERD, including heartburn and regurgitation. Plan is to start patient on a proton pump inhibitor (PPI) and recommend lifestyle modifications.\",\n",
        "    \"Procedure: Office hysteroscopy. The uterine cavity was visualized and appeared normal. No polyps or fibroids were seen.\",\n",
        "    \"Patient presents with a persistent cough. A chest x-ray was ordered, which showed evidence of bronchitis. No signs of pneumonia.\",\n",
        "    \"Procedure: Fine needle aspiration of a thyroid nodule. The procedure was performed under ultrasound guidance. Samples were sent for cytology.\",\n",
        "    # Summaries 31-40\n",
        "    \"Patient is a diabetic presenting for routine foot care. Physical exam shows no signs of ulceration or infection. Patient was educated on proper foot hygiene.\",\n",
        "    \"Procedure: Colposcopy with cervical biopsy. The procedure was performed due to an abnormal Pap smear. Biopsies were taken from the acetowhite areas.\",\n",
        "    \"Patient presents with knee pain. An MRI of the right knee was performed, which showed a medial meniscus tear.\",\n",
        "    \"Procedure: Myringotomy with tube insertion. A small incision was made in the tympanic membrane and a pressure equalization tube was placed.\",\n",
        "    \"Patient presents with anxiety. A discussion was held about treatment options, including therapy and medication. Patient agreed to start an SSRI.\",\n",
        "    \"Procedure: Incision and drainage of an abscess on the lower back. A significant amount of purulent material was drained. The wound was packed with iodoform gauze.\",\n",
        "    \"Patient presents with symptoms of a urinary tract infection (UTI). A urine sample was collected and showed evidence of infection. Antibiotics were prescribed.\",\n",
        "    \"Procedure: Shoulder arthroscopy with rotator cuff repair. A tear in the supraspinatus tendon was identified and repaired using suture anchors.\",\n",
        "    \"Patient presents for a well-child check. Growth and development are on track. All vaccinations are up to date.\",\n",
        "    \"Procedure: Esophagogastroduodenoscopy (EGD). Findings include mild gastritis and a small hiatal hernia. Biopsies were taken.\",\n",
        "    # Summaries 41-50\n",
        "    \"Patient presents with low back pain. Physical exam is consistent with muscle strain. Plan is to prescribe NSAIDs and recommend physical therapy.\",\n",
        "    \"Procedure: Carpal tunnel release. The transverse carpal ligament was incised to relieve pressure on the median nerve.\",\n",
        "    \"Patient presents with a skin rash. Physical exam suggests contact dermatitis. A topical steroid cream was prescribed.\",\n",
        "    \"Procedure: Closed reduction of a distal radius fracture. The fracture was successfully reduced and a cast was applied.\",\n",
        "    \"Patient presents with symptoms of depression. The PHQ-9 score was elevated. A plan was made to start psychotherapy and monitor symptoms.\",\n",
        "    \"Procedure: Cataract extraction with intraocular lens implantation. The procedure was successful and the patient's vision is expected to improve.\",\n",
        "    \"Patient presents with a sore throat. A rapid strep test was positive. Penicillin was prescribed.\",\n",
        "    \"Procedure: Coronary angiography. Findings show significant stenosis in the left anterior descending (LAD) artery. Plan is for percutaneous coronary intervention (PCI).\",\n",
        "    \"Patient presents for medication refill for hyperlipidemia. Recent lab work shows LDL cholesterol is at goal. Current statin therapy will be continued.\",\n",
        "    \"Procedure: Knee arthrocentesis. Synovial fluid was aspirated from the knee joint to relieve swelling and for analysis.\"\n",
        "]\n",
        "\n",
        "# Add the generated summaries as a new column in our sample DataFrame\n",
        "df_sample['summary'] = generated_summaries\n",
        "\n",
        "# --- Save the Final, AI-Ready File ---\n",
        "OUTPUT_FILE_TRANSCRIPTIONS = \"Transcriptions_MVP_Processed.csv\"\n",
        "df_sample.to_csv(OUTPUT_FILE_TRANSCRIPTIONS, index=False)\n",
        "\n",
        "print(f\"\\nüéâ PHASE 2 COMPLETE! The final, AI-ready transcription dataset has been saved as '{OUTPUT_FILE_TRANSCRIPTIONS}'.\")\n",
        "print(\"\\nHere is a final sample of the data, including the new 'summary' column:\")\n",
        "print(df_sample[['cleaned_transcription', 'summary']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0NUWEFj9xCIs"
      },
      "outputs": [],
      "source": [
        "# This line shows which file is needed\n",
        "TRANSCRIPTIONS_FILE = '/content/mtsamples.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0xDCcMHxhvE",
        "outputId": "a5d25538-6bc1-4fb0-c6da-a38b4d30146f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Loading and cleaning the transcriptions file...\n",
            "‚úÖ Initial cleaning complete.\n",
            "\n",
            "Step 2: Normalizing transcription text...\n",
            "‚úÖ Text normalization complete.\n",
            "\n",
            "Step 3: Creating the 'df_sample' DataFrame...\n",
            "‚úÖ 'df_sample' has been successfully created with 50 rows.\n"
          ]
        }
      ],
      "source": [
        "# --- PREREQUISITE CODE: RECREATE df_sample ---\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# PART 1: LOAD AND CLEAN THE TRANSCRIPTIONS DATA\n",
        "print(\"Step 1: Loading and cleaning the transcriptions file...\")\n",
        "TRANSCRIPTIONS_FILE = '/content/mtsamples.csv'\n",
        "try:\n",
        "    df_trans = pd.read_csv(TRANSCRIPTIONS_FILE)\n",
        "except FileNotFoundError:\n",
        "    print(f\"üõë Error: Make sure '{TRANSCRIPTIONS_FILE}' is uploaded to your Colab environment.\")\n",
        "    raise\n",
        "\n",
        "# Drop unnecessary columns, nulls, and duplicates\n",
        "df_clean = df_trans.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "df_clean.dropna(subset=['transcription'], inplace=True)\n",
        "df_clean.drop_duplicates(subset=['transcription'], inplace=True)\n",
        "df_final = df_clean[['medical_specialty', 'description', 'transcription']].copy()\n",
        "print(\"‚úÖ Initial cleaning complete.\")\n",
        "\n",
        "\n",
        "# PART 2: NORMALIZE THE TEXT\n",
        "print(\"\\nStep 2: Normalizing transcription text...\")\n",
        "def clean_transcription_text(text):\n",
        "    \"\"\"Cleans raw transcription text by removing headers and extra whitespace.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = re.sub(r'[A-Z\\s]+:', '', text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df_final['cleaned_transcription'] = df_final['transcription'].apply(clean_transcription_text)\n",
        "print(\"‚úÖ Text normalization complete.\")\n",
        "\n",
        "\n",
        "# PART 3: CREATE THE df_sample VARIABLE (THIS FIXES THE ERROR)\n",
        "print(\"\\nStep 3: Creating the 'df_sample' DataFrame...\")\n",
        "SAMPLE_SIZE = 50\n",
        "df_sample = df_final.head(SAMPLE_SIZE).copy()\n",
        "print(f\"‚úÖ 'df_sample' has been successfully created with {len(df_sample)} rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CesIBrVzsyrJ"
      },
      "source": [
        "# Phase 3.1: Secure Anonymization and Text Chunking for RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSB9OqHfs3fU"
      },
      "source": [
        "This cell prepares the text for indexing. First, it securely anonymizes Patient IDs using a \"salt\" (a secret key) that you will store safely in Colab's secrets manager. Then, it breaks down long transcriptions into smaller, overlapping \"chunks.\" This is critical for the AI to find specific details accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zbm6EC5s18M",
        "outputId": "d66d54ab-8d32-466c-cef1-7a395f1ee395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Text chunking complete. Created 210 chunks from 50 documents.\n",
            "Sample chunk:\n",
            "   original_doc_id chunk_id                                         chunk_text\n",
            "0                0      0_0  ,  This 23-year-old white female presents with...\n",
            "1                0      0_1  , , Her only medication currently is Ortho Tri...\n",
            "2                0      0_2  ostril given for three weeks.  A prescription ...\n",
            "3                1      1_0  , He has difficulty climbing stairs, difficult...\n",
            "4                1      1_1   months ago.  He now smokes less than three ci...\n"
          ]
        }
      ],
      "source": [
        "# --- PHASE 3.1: SECURE ANONYMIZATION AND TEXT CHUNKING ---\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "# --- URGENT SECURITY STEP ---\n",
        "# 1. In Google Colab, click the \"Key\" icon on the left panel.\n",
        "# 2. Click \"+ Add new secret\".\n",
        "# 3. For the name, enter: MEDIMINDER_SALT\n",
        "# 4. For the value, enter a long, random secret string.\n",
        "# 5. Make sure \"Notebook access\" is toggled ON.\n",
        "# 6. Re-run this cell.\n",
        "from google.colab import userdata\n",
        "salt = userdata.get('MEDIMINDER_SALT')\n",
        "\n",
        "if not salt:\n",
        "    raise ValueError(\"CRITICAL: Secret 'MEDIMINDER_SALT' not found. Please add it in Colab's secrets panel.\")\n",
        "\n",
        "def hash_id(patient_id):\n",
        "    \"\"\"Creates a secure, non-reversible hash for the patient ID.\"\"\"\n",
        "    return hashlib.sha256((str(patient_id) + salt).encode()).hexdigest()[:16]\n",
        "\n",
        "# Apply the secure hash to your merged Synthea data (if not already done)\n",
        "# df_merged['PATIENT_ID_HASHED'] = df_merged['PATIENT_ID'].apply(hash_id)\n",
        "\n",
        "# --- Text Chunking for RAG ---\n",
        "def chunk_text(text, chunk_size=750, overlap=150):\n",
        "    \"\"\"Splits a long text into smaller, overlapping chunks.\"\"\"\n",
        "    if not isinstance(text, str): return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += (chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "# Create a new DataFrame with one row per chunk\n",
        "# df_sample is the DataFrame with 50 transcriptions from your Phase 2\n",
        "chunked_rows = []\n",
        "for idx, row in df_sample.iterrows():\n",
        "    text_chunks = chunk_text(row['cleaned_transcription'])\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        chunked_rows.append({\n",
        "            'original_doc_id': idx, # Links back to the original transcription\n",
        "            'chunk_id': f\"{idx}_{i}\",\n",
        "            'chunk_text': chunk\n",
        "        })\n",
        "\n",
        "df_chunked = pd.DataFrame(chunked_rows)\n",
        "print(f\"‚úÖ Text chunking complete. Created {len(df_chunked)} chunks from {len(df_sample)} documents.\")\n",
        "print(\"Sample chunk:\")\n",
        "print(df_chunked.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOaMBKyqs-Xf"
      },
      "source": [
        "# Phase 3.2: Create Production-Grade FAISS Index with Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5B7CPBctAK5"
      },
      "source": [
        "This cell converts the text chunks into numerical vectors (embeddings). It then builds a FAISS index, which is like a high-speed search engine for these vectors. Crucially, it maps each vector back to its original document and chunk ID, so we always know where the information came from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuZO1xJl9Vx-",
        "outputId": "0400c05f-7990-47f7-f3d1-604772832cfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "46a3490b23c9439eae5f96f0bbab1e96",
            "a4e126a5bf444d30b7a2ceb95100346d",
            "81e1aa77d89d4bbcb071c135a782b78d",
            "fbab9c3ef1bc456d982fd1cb2e9d5966",
            "cde688d38870461ca70055182d21a9de",
            "29caf5db7f0d4b44a2affe156b04f743",
            "4e6dd03184424dc2b0bb50c478984a13",
            "19015dad556c4401b7098520c0041e03",
            "c394582bb26a4a158b9875824b855a88",
            "6f3c4769bafc4d76bb3d252647131d5f",
            "e6569fdd1e4a43528367ac5089e77b95"
          ]
        },
        "id": "nLcuxupGs_rc",
        "outputId": "f6df23f3-2e52-4121-f869-f8b56eeb8988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vector embeddings for all text chunks...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46a3490b23c9439eae5f96f0bbab1e96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ FAISS index created successfully with 210 vectors.\n",
            "\n",
            "Artifacts saved to your Colab environment:\n",
            "- transcriptions_index.faiss\n",
            "- chunked_transcriptions.parquet\n"
          ]
        }
      ],
      "source": [
        "# --- PHASE 3.2: CREATE PRODUCTION-GRADE FAISS INDEX (CORRECTED) ---\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load the embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Get the list of chunked texts to be indexed\n",
        "# Ensure df_chunked exists from the previous step\n",
        "texts_to_embed = df_chunked['chunk_text'].tolist()\n",
        "\n",
        "# 3. Create the embeddings\n",
        "print(\"Creating vector embeddings for all text chunks...\")\n",
        "embeddings = model.encode(texts_to_embed, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
        "# --- THIS IS THE CORRECTED LINE ---\n",
        "embeddings = embeddings.astype(np.float32) # Using the actual data type, not a string\n",
        "\n",
        "# 4. Normalize the vectors\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "# 5. Build the FAISS index with ID mapping\n",
        "index_dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(index_dimension)\n",
        "index_with_ids = faiss.IndexIDMap(index)\n",
        "# Also using the correct data type here for robustness\n",
        "ids = np.arange(len(df_chunked)).astype(np.int64)\n",
        "index_with_ids.add_with_ids(embeddings, ids)\n",
        "\n",
        "print(f\"\\n‚úÖ FAISS index created successfully with {index_with_ids.ntotal} vectors.\")\n",
        "\n",
        "# 6. Save the final artifacts\n",
        "FAISS_INDEX_FILE = \"transcriptions_index.faiss\"\n",
        "CHUNKED_DATA_FILE = \"chunked_transcriptions.parquet\"\n",
        "\n",
        "faiss.write_index(index_with_ids, FAISS_INDEX_FILE)\n",
        "df_chunked.to_parquet(CHUNKED_DATA_FILE, index=False)\n",
        "\n",
        "print(f\"\\nArtifacts saved to your Colab environment:\")\n",
        "print(f\"- {FAISS_INDEX_FILE}\")\n",
        "print(f\"- {CHUNKED_DATA_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnv6mXmxtEpi"
      },
      "source": [
        "# Phase 3.3: Final RAG Validation Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-E5w32ttGhj"
      },
      "source": [
        "This cell performs the final \"RAG validation\" requested. It simulates a user query, uses the new index to retrieve the most relevant text chunks, and injects them into the official prompt template. This proves your data pipeline works end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLB4KGM_tFOr",
        "outputId": "cfefd969-8956-4038-95e2-835c3fce64ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- RAG VALIDATION COMPLETE ---\n",
            "\n",
            "This test proves that your saved artifacts work correctly.\n",
            "\n",
            "Final Prompt Ready for the LLM:\n",
            "------------------------------\n",
            "\n",
            "You are a compassionate and professional healthcare provider.\n",
            "You are summarizing a patient‚Äôs recent doctor visit.\n",
            "\n",
            "Your goal:\n",
            "- Be warm, kind, and reassuring.\n",
            "- Use clear, simple language suitable for a patient.\n",
            "- Greet the patient at the start (‚ÄúHello there,‚Äù).\n",
            "- Summarize key findings, recommendations, and next steps.\n",
            "- End with a caring reminder or motivational note.\n",
            "\n",
            "Context from doctor‚Äôs note or visit summary:\n",
            ", , Her only medication currently is Ortho Tri-Cyclen and the Allegra., , She has no known medicine allergies.,,Vitals:  Weight was 130 pounds and blood pressure 124/78.,  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,,  Allergic rhinitis.,,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n",
            "\n",
            ",  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up., , Her only medication currently is Ortho Tri-Cyclen and the Allegra., , She has no known medicine allergies.,,Vitals:  Weight was 130 pounds and blo\n",
            "\n",
            ",  The patient is a 17-year-old female, who presents to the emergency room with foreign body and airway compromise and was taken to the operating room.  She was intubated and fishbone., , Significant for diabetes, hypertension, asthma, cholecystectomy, and total hysterectomy and cataract.,  ,No known drug allergies., , Prevacid, Humulin, Diprivan, Proventil, Unasyn, and Solu-Medrol., , Noncontributory., , Negative for illicit drugs, alcohol, and tobacco.,  ,Please see the hospital chart., , Please see the hospital chart., , The patient was taken to the operating room by Dr. X who is covering for ENT and noted that she had airway compromise and a rather large fishbone noted and that was removed.  The patient was intubated and it was felt tha\n",
            "\n",
            "Now write a short, empathetic summary message for the patient.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- PHASE 3.3: FINAL RAG VALIDATION TEST ---\n",
        "import faiss\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load the artifacts you just created\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "index = faiss.read_index(\"transcriptions_index.faiss\")\n",
        "df_chunked = pd.read_parquet(\"chunked_transcriptions.parquet\")\n",
        "\n",
        "# 2. Define the RAG Prompt Template \n",
        "rag_prompt_template = \"\"\"\n",
        "You are a compassionate and professional healthcare provider.\n",
        "You are summarizing a patient‚Äôs recent doctor visit.\n",
        "\n",
        "Your goal:\n",
        "- Be warm, kind, and reassuring.\n",
        "- Use clear, simple language suitable for a patient.\n",
        "- Greet the patient at the start (‚ÄúHello there,‚Äù).\n",
        "- Summarize key findings, recommendations, and next steps.\n",
        "- End with a caring reminder or motivational note.\n",
        "\n",
        "Context from doctor‚Äôs note or visit summary:\n",
        "{{retrieved_context}}\n",
        "\n",
        "Now write a short, empathetic summary message for the patient.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Simulate a user query and retrieve context\n",
        "query = \"What did the doctor say about my allergies and Nasonex?\"\n",
        "query_embedding = model.encode([query], convert_to_numpy=True).astype(np.float32)\n",
        "faiss.normalize_L2(query_embedding)\n",
        "\n",
        "k = 3 # Retrieve the top 3 most relevant chunks\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "retrieved_chunks = df_chunked.iloc[indices[0]]\n",
        "retrieved_context = \"\\n\\n\".join(retrieved_chunks['chunk_text'].tolist())\n",
        "\n",
        "# 4. Inject the context into the prompt\n",
        "final_prompt_for_llm = rag_prompt_template.replace(\"{{retrieved_context}}\", retrieved_context)\n",
        "\n",
        "print(\"--- RAG VALIDATION COMPLETE ---\")\n",
        "print(\"\\nThis test proves that your saved artifacts work correctly.\")\n",
        "print(\"\\nFinal Prompt Ready for the LLM:\")\n",
        "print(\"-\" * 30)\n",
        "print(final_prompt_for_llm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19015dad556c4401b7098520c0041e03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29caf5db7f0d4b44a2affe156b04f743": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a3490b23c9439eae5f96f0bbab1e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4e126a5bf444d30b7a2ceb95100346d",
              "IPY_MODEL_81e1aa77d89d4bbcb071c135a782b78d",
              "IPY_MODEL_fbab9c3ef1bc456d982fd1cb2e9d5966"
            ],
            "layout": "IPY_MODEL_cde688d38870461ca70055182d21a9de"
          }
        },
        "4e6dd03184424dc2b0bb50c478984a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f3c4769bafc4d76bb3d252647131d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e1aa77d89d4bbcb071c135a782b78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19015dad556c4401b7098520c0041e03",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c394582bb26a4a158b9875824b855a88",
            "value": 4
          }
        },
        "a4e126a5bf444d30b7a2ceb95100346d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29caf5db7f0d4b44a2affe156b04f743",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e6dd03184424dc2b0bb50c478984a13",
            "value": "Batches:‚Äá100%"
          }
        },
        "c394582bb26a4a158b9875824b855a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cde688d38870461ca70055182d21a9de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6569fdd1e4a43528367ac5089e77b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbab9c3ef1bc456d982fd1cb2e9d5966": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f3c4769bafc4d76bb3d252647131d5f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e6569fdd1e4a43528367ac5089e77b95",
            "value": "‚Äá4/4‚Äá[00:23&lt;00:00,‚Äá‚Äá4.83s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
